{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO31dmwOUARAxVdspjNrymK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamudralaAnuhya/vectorization/blob/main/Vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E-LnEdasJnSi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "comparing the times between for loop and vector implmentation"
      ],
      "metadata": {
        "id": "qENypsQvK9XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.random.rand(1000000 )\n",
        "b = np.random.rand(1000000)\n",
        "\n",
        "tic = time.time()\n",
        "c = np.dot(a,b)\n",
        "toc = time.time()\n",
        "\n",
        "print(c)\n",
        "print(\"vectorized implementation:\" + str(1000*(toc-tic)) + \"ms\" )\n",
        "\n",
        "c=0\n",
        "tic = time.time()\n",
        "for i in range (1000000):\n",
        "  c += a[i]*b[i]\n",
        "toc = time.time()\n",
        "print(c)\n",
        "print(\"For loop implementation:\" + str(1000*(toc-tic)) + \"ms\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImkIoLFdJtmO",
        "outputId": "dcf34041-2c50-497d-dfbb-bc201c8f63f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249974.0328730776\n",
            "vectorized implementation:1.3649463653564453ms\n",
            "249974.03287307644\n",
            "For loop implementation:581.5060138702393ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Broadcasting"
      ],
      "metadata": {
        "id": "CywSfhx5SPk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[1,2,3,4],\n",
        "             [5,6,7,8],\n",
        "             [9,10,11,12]])"
      ],
      "metadata": {
        "id": "X24Hs7rCSOKB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal = A.sum(axis = 0)\n",
        "print(cal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW9umolrSOP5",
        "outputId": "bd029ef7-2918-4a23-d81c-4f3de4c77fb6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15 18 21 24]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentage = 100*A/cal.reshape(1,4)\n",
        "print(percentage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBwdPB_TSOUO",
        "outputId": "71e2b2ca-7281-424b-83cf-6592f0568e73"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 6.66666667 11.11111111 14.28571429 16.66666667]\n",
            " [33.33333333 33.33333333 33.33333333 33.33333333]\n",
            " [60.         55.55555556 52.38095238 50.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a= np.random.randn(10 , 1)\n",
        "b = np.random.rand(1, 2)\n",
        "print(a,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "123AWikUSOWx",
        "outputId": "287ddbe1-507b-4374-fb01-7a606b9b235c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.16342905]\n",
            " [ 0.00645427]\n",
            " [-0.526433  ]\n",
            " [-0.89928789]\n",
            " [ 1.01516892]\n",
            " [-0.70172376]\n",
            " [-1.98638494]\n",
            " [-0.24042908]\n",
            " [ 0.01802552]\n",
            " [-2.58690105]] [[0.9905157  0.62464977]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=np.random.randn(3,3)\n",
        "b=np.random.randn(3,1)\n",
        "c = a * b\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEj5zxBgUw0j",
        "outputId": "572a0372-451f-4f1a-f211-e3d885e48bae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.08589326 -0.57961489 -1.10759195]\n",
            " [-0.23319104 -0.24640737 -0.07718295]\n",
            " [-0.07797186 -0.13080228 -0.16224225]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lineaer regression\n",
        "import numpy as np\n",
        "\n",
        "# Manually defining x (images) and y (labels)\n",
        "X = np.array([\n",
        "    [[82, 71, 68], [89, 83, 83], [100, 98, 104], [131, 132, 137], [126, 124, 124], [105, 97, 95]],\n",
        "    [[95, 91, 97], [104, 104, 113], [110, 115, 126], [135, 134, 135], [127, 122, 119], [111, 105, 103]],\n",
        "    [[94, 85, 83], [97, 89, 90], [110, 109, 115], [136, 134, 131], [127, 120, 117], [116, 108, 104]],\n",
        "    [[96, 116, 131], [97, 115, 130], [103, 123, 139], [152, 155, 157], [146, 149, 152], [130, 133, 134]],\n",
        "    [[90, 108, 123], [92, 108, 121], [100, 119, 134], [150, 152, 155], [144, 146, 147], [134, 135, 134]],\n",
        "    [[86, 102, 116], [87, 103, 115], [94, 114, 127], [154, 156, 160], [146, 148, 152], [138, 141, 142]]\n",
        "], dtype='uint8')\n",
        "\n",
        "y = np.array([1, 1])  # Labels for these two images (both cats)\n"
      ],
      "metadata": {
        "id": "y2gw22B6Ye6a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n",
        "    \"\"\"\n",
        "    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples)\n",
        "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n",
        "    learning_rate -- learning rate of the optimization\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- If True, print the cost every 10000 iterations\n",
        "    lambd -- regularization hyperparameter, scalar\n",
        "    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n",
        "\n",
        "    Returns:\n",
        "    parameters -- parameters learned by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    grads = {}\n",
        "    costs = []                            # to keep track of the cost\n",
        "    m = X.shape[1]                        # number of examples\n",
        "    layers_dims = [X.shape[0], 20, 3, 1]\n",
        "\n",
        "    # Initialize parameters dictionary.\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
        "        if keep_prob == 1:\n",
        "            a3, cache = forward_propagation(X, parameters)\n",
        "        elif keep_prob < 1:\n",
        "            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
        "\n",
        "        # Cost function\n",
        "        if lambd == 0:\n",
        "            cost = compute_cost(a3, Y)\n",
        "        else:\n",
        "            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n",
        "\n",
        "        # Backward propagation.\n",
        "        assert (lambd == 0 or keep_prob == 1)   # it is possible to use both L2 regularization and dropout,\n",
        "                                                # but this assignment will only explore one at a time\n",
        "        if lambd == 0 and keep_prob == 1:\n",
        "            grads = backward_propagation(X, Y, cache)\n",
        "        elif lambd != 0:\n",
        "            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
        "        elif keep_prob < 1:\n",
        "            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
        "\n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        # Print the loss every 10000 iterations\n",
        "        if print_cost and i % 10000 == 0:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "    # plot the cost\n",
        "    plt.plot(costs)\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (x1,000)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "2YRZxyGvJoIo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "jx5HgP2OPplZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Th edata set for any the image will be seen like above for example [82, 71, 68]....red , blue, green and its like 3 set of values we need to make into 1 set\n",
        "\n",
        "m = X.shape[1] #input values size\n",
        "W = np.zeros(X.shape[0]) #1d array to add the values for the difference between predictionvalue and exact value\n",
        "b = 0.0 #scalar value for small changes\n",
        "#flatten\n",
        "X_flatten = X.reshape(X.shape[0], -1).T    #FLatten dataset\n",
        "#sigmoid (Activation function)\n",
        "A = 1/(1+np.exp(-(np.dot(W.T , X) +b )))   #Sigmoid (logistic Regression using sigmoid function we need to find w,b). ... in finel I need to do this\n",
        "#cost function\n",
        "cost = -(1/m) * np.sum((Y * np.log(A))+ ((1-Y) * np.log(1-A))) #gradient descent loss function (Sum of loss is cost)\n",
        "cost = np.squeeze(np.array(cost)) # remove dimensional array and make it as scalar\n",
        "\n",
        "#we need to find w , b  .....w = w-alphs(dw)  ; b = b-alpha(db)\n",
        "dw = (1 / m) * np.dot(X, (A - Y).T) #∂𝐽∂𝑤=1𝑚𝑋(𝐴−𝑌)𝑇(7)\n",
        "db = (1/m) *np.sum(A-Y) #∂𝐽∂𝑏=1𝑚∑𝑖=1𝑚(𝑎(𝑖)−𝑦(𝑖))\n",
        "\n",
        "#optimize\n",
        "leanring_rate = 0.09\n",
        "for i in range (100):   # here we should give number base on how many iterations should run for to reduce w,b values and give good prediction\n",
        "  w = w - leanring_rate(dw)\n",
        "  b = b - learning_rate(db)\n",
        "\n",
        "#prediction\n",
        "for i in range(A.shape[1]):\n",
        "  if A[0,i] > 0.5:\n",
        "    Y_prediction[0,i] =  1\n",
        "  else :\n",
        "    Y_prediction[0,i] = 0\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9aecNEyMJoNL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "initialization"
      ],
      "metadata": {
        "id": "N20y70jA9nuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#deeplearning model\n",
        "#Hyperparameter tuning ---Random initialization\n",
        "\n",
        "#initialzing first w,b\n",
        "# Zeros initialization -- setting initialization = \"zeros\" in the input argument.\n",
        "# Random initialization -- setting initialization = \"random\" in the input argument. This initializes the weights to large random values.\n",
        "# He initialization -- setting initialization = \"he\" in the input argument. This initializes the weights to random values  (relu)\n",
        "\n",
        "#zero initialization (the cost doesn't decrease, and the algorithm performs no better than random guessing)\n",
        "L = len(layers_dims) - 1\n",
        "for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))\n",
        "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
        "    return parameters\n",
        "\n",
        "#initialize_parameters_random (multiply with 0.01 to  any but small makes better but how small it shoulb be ? here comes next he(relu))\n",
        " for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
        "    return parameters\n",
        "\n",
        "\n",
        "#He Initialization(Relu)sqrt(2/previous layer)(recommended method)\n",
        "for l in range(1, L + 1):\n",
        "        parameters['W' + str(l)] = np.random.randn(layers_dims[l] ,layers_dims[l-1])*np.sqrt(2/layers_dims[l-1])\n",
        "        parameters['b' + str(l)] =np.zeros((layers_dims[l] ,1))\n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "nGVT97EOJoR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regularization ---adding lambda (lambd) in the cost function\n",
        "two techniques - l2 regularization and dropout\n",
        "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
        "\n",
        "#Dropout - decrease number of neurons in the layer\n",
        "\n",
        "\n",
        "L2 regularization -- functions: \"compute_cost_with_regularization()\" and \"backward_propagation_with_regularization()\"\n",
        "Dropout -- functions: \"forward_propagation_with_dropout()\" and \"backward_propagation_with_dropout()\"\n"
      ],
      "metadata": {
        "id": "deoyeDMo9r4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What you should remember: the implications of L2-regularization on:\n",
        "\n",
        "# The cost computation:\n",
        "# A regularization term is added to the cost.\n",
        "# The backpropagation function:\n",
        "# There are extra terms in the gradients with respect to weight matrices.\n",
        "# Weights end up smaller (\"weight decay\"):\n",
        "# Weights are pushed to smaller values.\n",
        "\n",
        "\n",
        "L2_regularization_cost = (lambd /  (2 * m))* (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\n",
        "\n",
        "\n",
        "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
        "    \"\"\"\n",
        "    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input dataset, of shape (input size, number of examples)\n",
        "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
        "    cache -- cache output from forward_propagation()\n",
        "    lambd -- regularization hyperparameter, scalar\n",
        "\n",
        "    Returns:\n",
        "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[1]\n",
        "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "\n",
        "     # Compute gradients for the output layer\n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = 1./m * np.dot(dZ3, A2.T) + (lambd / m) * W3\n",
        "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
        "\n",
        "     # Propagate to the second layer\n",
        "    dA2 = np.dot(W3.T, dZ3)\n",
        "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
        "    dW2 = 1./m * np.dot(dZ2, A1.T) + (lambd / m) * W2\n",
        "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "      # Propagate to the first layer\n",
        "    dA1 = np.dot(W2.T, dZ2)\n",
        "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
        "    dW1 = 1. / m * np.dot(dZ1, X.T) + (lambd / m) * W1\n",
        "    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
        "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1,\n",
        "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
        "\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "dK1xG4zDJoWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropout...works based on keep_prob\n",
        "# **What you should remember about dropout:**\n",
        "# - Dropout is a regularization technique.\n",
        "# - You only use dropout during training. Don't use dropout (randomly eliminate nodes) during test time.\n",
        "# - Apply dropout both during forward and backward propagation.\n",
        "# - During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2\n",
        "#  Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.\n",
        "\n",
        "# 4 steps for each forward layer\n",
        "# D1 = np.random.rand(A1.shape[0],A1.shape[1])  # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
        "#     D1 = (D1 < keep_prob).astype(int)             # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
        "#     A1 =  A1 * D1                                 # Step 3: shut down some neurons of A1\n",
        "#     A1 =  A1 / keep_prob\n",
        "\n",
        "#for backward layer\n",
        "    # dZ3 = A3 - Y\n",
        "    # dW3 = 1./m * np.dot(dZ3, A2.T)\n",
        "    # db3 = 1./m * np.sum(dZ3, axis=1, keepdims=True)\n",
        "    # dA2 = np.dot(W3.T, dZ3)\n",
        "    # dA2 = dA2 * D2                # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n",
        "    # dA2 = dA2 / keep_prob\n",
        "\n",
        "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input dataset, of shape (2, number of examples)\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
        "                    W1 -- weight matrix of shape (20, 2)\n",
        "                    b1 -- bias vector of shape (20, 1)\n",
        "                    W2 -- weight matrix of shape (3, 20)\n",
        "                    b2 -- bias vector of shape (3, 1)\n",
        "                    W3 -- weight matrix of shape (1, 3)\n",
        "                    b3 -- bias vector of shape (1, 1)\n",
        "    keep_prob - probability of keeping a neuron active during drop-out, scalar\n",
        "\n",
        "    Returns:\n",
        "    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n",
        "    cache -- tuple, information stored for computing the backward propagation\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "\n",
        "    # retrieve parameters\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    W3 = parameters[\"W3\"]\n",
        "    b3 = parameters[\"b3\"]\n",
        "\n",
        "\n",
        "    # Dropout for Layer 1\n",
        "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = relu(Z1)\n",
        "    #(≈ 4 lines of code)         # Steps 1-4 below correspond to the Steps 1-4 described above.\n",
        "    D1 = np.random.rand(A1.shape[0],A1.shape[1])  # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
        "    D1 = (D1 < keep_prob).astype(int)             # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
        "    A1 =  A1 * D1                                 # Step 3: shut down some neurons of A1\n",
        "    A1 =  A1 / keep_prob                          # Step 4: scale the value of neurons that haven't been shut down\n",
        "\n",
        "    # Dropout for Layer 1\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = relu(Z2)\n",
        "    #(≈ 4 lines of code)\n",
        "    D2 = np.random.rand(A2.shape[0],A2.shape[1])  # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n",
        "    D2 = (D2 < keep_prob).astype(int)             # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)\n",
        "    A2 =  A2 * D2                                 # Step 3: shut down some neurons of A2\n",
        "    A2 =  A2 / keep_prob                          # Step 4: scale the value of neurons that haven't been shut down\n",
        "\n",
        "    Z3 = np.dot(W3, A2) + b3\n",
        "    A3 = sigmoid(Z3)\n",
        "\n",
        "    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
        "\n",
        "    return A3, cache"
      ],
      "metadata": {
        "id": "QJANFSFTJoYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Checking\n",
        "2 - Problem Statement\n",
        "\n",
        "You are part of a team working to make mobile payments available globally, and are asked to build a deep learning model to detect fraud--whenever someone makes a payment, you want to see if the payment might be fraudulent, such as if the user's account has been taken over by a hacker.\n",
        "\n",
        "You already know that backpropagation is quite challenging to implement, and sometimes has bugs. Because this is a mission-critical application, your company's CEO wants to be really certain that your implementation of backpropagation is correct. Your CEO says, \"Give me proof that your backpropagation is actually working!\" To give this reassurance, you are going to use \"gradient checking.\"\n",
        "\n",
        "Let's do it!"
      ],
      "metadata": {
        "id": "wpl6sTlrAOXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First compute \"gradapprox\" using the formula above (1) and a small value of $\\varepsilon$. Here are the Steps to follow:\n",
        "#     1. $\\theta^{+} = \\theta + \\varepsilon$\n",
        "#     2. $\\theta^{-} = \\theta - \\varepsilon$\n",
        "#     3. $J^{+} = J(\\theta^{+})$\n",
        "#     4. $J^{-} = J(\\theta^{-})$\n",
        "#     5. $gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}$\n",
        "# - Then compute the gradient using backward propagation, and store the result in a variable \"grad\"\n",
        "# - Finally, compute the relative difference between \"gradapprox\" and the \"grad\" using the following formula:\n",
        "# $$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$\n",
        "# You will need 3 Steps to compute this formula:\n",
        "#    - 1'. compute the numerator using np.linalg.norm(...)\n",
        "#    - 2'. compute the denominator. You will need to call np.linalg.norm(...) twice.\n",
        "#    - 3'. divide them.\n",
        "# - If this difference is small (say less than $10^{-7}$), you can be quite confident that you have computed your gradient correctly. Otherwise, there may be a mistake in the gradient computation.\n",
        "\n",
        "\n",
        "# GRADED FUNCTION: gradient_check_n\n",
        "\n",
        "def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7, print_msg=False):\n",
        "    \"\"\"\n",
        "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
        "    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters\n",
        "    X -- input datapoint, of shape (input size, number of examples)\n",
        "    Y -- true \"label\"\n",
        "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
        "\n",
        "    Returns:\n",
        "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
        "    \"\"\"\n",
        "\n",
        "    # Set-up variables\n",
        "    parameters_values, _ = dictionary_to_vector(parameters)\n",
        "\n",
        "    grad = gradients_to_vector(gradients)\n",
        "    num_parameters = parameters_values.shape[0]\n",
        "    J_plus = np.zeros((num_parameters, 1))\n",
        "    J_minus = np.zeros((num_parameters, 1))\n",
        "    gradapprox = np.zeros((num_parameters, 1))\n",
        "\n",
        "    # Compute gradapprox\n",
        "    for i in range(num_parameters):\n",
        "\n",
        "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
        "        # \"_\" is used because the function you have outputs two parameters but we only care about the first one\n",
        "        #(approx. 3 lines)\n",
        "        theta_plus = np.copy(parameters_values)                      # Step 1\n",
        "        theta_plus[i] += epsilon                                    # Step 2\n",
        "        J_plus[i], _ =   forward_propagation_n(X, Y, vector_to_dictionary(theta_plus))                                  # Step 3\n",
        "\n",
        "\n",
        "        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
        "        #(approx. 3 lines)\n",
        "        theta_minus = np.copy(parameters_values)                # Step 1\n",
        "        theta_minus[i] -= epsilon                                 # Step 2\n",
        "        J_minus[i], _ =   forward_propagation_n(X, Y, vector_to_dictionary(theta_minus))                              # Step 3\n",
        "\n",
        "\n",
        "        # Compute gradapprox[i]\n",
        "        # (approx. 1 line)\n",
        "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
        "\n",
        "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
        "    # (approx. 3 line)\n",
        "#     numerator = np.linalg.norm(grad - gradapprox)                    # Step 1'\n",
        "#     denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)  # Step 2'\n",
        "#     difference = numerator / denominator\n",
        "    numerator = np.linalg.norm(grad - gradapprox)           # Step 1': Compute the numerator (Euclidean norm of the difference)\n",
        "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)  # Step 2': Compute the denominator\n",
        "    difference = numerator / denominator# Step 3'\n",
        "\n",
        "    if print_msg:\n",
        "        if difference > 2e-7:\n",
        "            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
        "        else:\n",
        "            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
        "\n",
        "    return difference"
      ],
      "metadata": {
        "id": "kvieI5d2AaR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vqlhSC-4AaUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types of gradient descent\n",
        " 1. Stochastic Gradient Descent .. it works each and evry value in the dataset and (online learning or ads)\n",
        " 2. Mini-Batch Gradient Descent ...divide the data into batches and compute the gradient descent (gives good accuaracy and less time)\n",
        " epoch ...completing of all values in dataset in the batches is 1 epoche\n",
        " 3. Batch Gradient Descent: (works on all elements on dataset and update gradient descent (slow and good accuracy)"
      ],
      "metadata": {
        "id": "dadZrclOsvjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch\n",
        "X = data_input\n",
        "Y = labels\n",
        "m = X.shape[1]  # Number of training examples\n",
        "parameters = initialize_parameters(layers_dims)\n",
        "for i in range(0, num_iterations):\n",
        "    # Forward propagation\n",
        "    a, caches = forward_propagation(X, parameters)\n",
        "    # Compute cost\n",
        "    cost_total = compute_cost(a, Y)  # Cost for m training examples\n",
        "    # Backward propagation\n",
        "    grads = backward_propagation(a, caches, parameters)\n",
        "    # Update parameters\n",
        "    parameters = update_parameters(parameters, grads)\n",
        "    # Compute average cost\n",
        "    cost_avg = cost_total / m\n",
        "\n",
        "\n",
        "# Stochastic Gradient Descent:\n",
        "X = data_input\n",
        "Y = labels\n",
        "m = X.shape[1]  # Number of training examples\n",
        "parameters = initialize_parameters(layers_dims)\n",
        "for i in range(0, num_iterations):\n",
        "    cost_total = 0\n",
        "    for j in range(0, m):\n",
        "        # Forward propagation\n",
        "        a, caches = forward_propagation(X[:,j], parameters)\n",
        "        # Compute cost\n",
        "        cost_total += compute_cost(a, Y[:,j])  # Cost for one training example\n",
        "        # Backward propagation\n",
        "        grads = backward_propagation(a, caches, parameters)\n",
        "        # Update parameters\n",
        "        parameters = update_parameters(parameters, grads)\n",
        "    # Compute average cost\n",
        "    cost_avg = cost_total / m\n",
        "\n",
        "\n",
        "#minibatch\n",
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples)\n",
        "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
        "    mini_batch_size -- size of the mini-batches, integer\n",
        "\n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
        "    m = X.shape[1]                  # number of training examples\n",
        "    mini_batches = []\n",
        "\n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
        "\n",
        "    inc = mini_batch_size\n",
        "\n",
        "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
        "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
        "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        # (approx. 2 lines)\n",
        "        mini_batch_X =  shuffled_X[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n",
        "        mini_batch_Y =shuffled_Y[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "\n",
        "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
        "    if m % mini_batch_size != 0:\n",
        "        #(approx. 2 lines)\n",
        "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size:]\n",
        "        mini_batch_Y =shuffled_Y[:, num_complete_minibatches * mini_batch_size:]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "\n",
        "    return mini_batches\n",
        "\n"
      ],
      "metadata": {
        "id": "PU9a2599AaXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8XkOvrrFAaZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimization **   - The process of minimizing (or maximizing) a loss function to find the optimal model parameters J(w,b)\n",
        "\n",
        "hyperparameters  = learning rate (alpha) , momentum (beta1 , beta2) , batchsize\n",
        "\n",
        "**Momentum** - \tAccelerates gradient descent by adding a fraction of the previous gradient to the current update (V_DW , V_DB)\n",
        "\n",
        "$$ \\begin{cases}\n",
        "v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\n",
        "W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n",
        "\\end{cases}\\tag{1}$$\n",
        "\n",
        "$$\\begin{cases}\n",
        "v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\n",
        "b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}}\n",
        "\\end{cases}\\tag{2}$$\n",
        "\n",
        "**RMSProp **- Scales learning rate for each parameter based on the magnitude of recent gradients (S_DW , S_DB)\n",
        "\n",
        "$$ \\begin{cases}\n",
        "s_{dW^{[l]}} = \\beta s_{dW^{[l]}} + (1 - \\beta) dW^{[l]**2} \\\\\n",
        "W^{[l]} = W^{[l]} - \\alpha S_{dW^{[l] }}\n",
        "\\end{cases}\\tag{3}$$\n",
        "\n",
        "$$\\begin{cases}\n",
        "S_{db^{[l]}} = \\beta S_{db^{[l]}} + (1 - \\beta) db^{[l]**2} \\\\\n",
        "b^{[l]} = b^{[l]} - \\alpha S_{db^{[l]}}\n",
        "\\end{cases}\\tag{4}$$\n",
        "\n",
        "\n",
        "\n",
        "**Adam **- Combines the advantages of Momentum and RMSProp to adapt learning rates for each parameter\n",
        "$$\\begin{cases}\n",
        "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n",
        "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
        "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n",
        "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
        "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n",
        "\\end{cases}$$\n"
      ],
      "metadata": {
        "id": "P_Kq_PvKuM9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
        "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
        "    \"\"\"\n",
        "    Update parameters using Adam\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    t -- Adam variable, counts the number of taken steps\n",
        "    learning_rate -- the learning rate, scalar.\n",
        "    beta1 -- Exponential decay hyperparameter for the first moment estimates\n",
        "    beta2 -- Exponential decay hyperparameter for the second moment estimates\n",
        "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "\n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(1, L + 1):\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        "        # (approx. 2 lines)\n",
        "        v[\"dW\" + str(l)] = beta1 * v[\"dW\" + str(l)] + (1-beta1) * grads[\"dW\" + str(l)]\n",
        "        v[\"db\" + str(l)] = beta1 * v[\"db\" + str(l)] + (1-beta1) * grads[\"db\" + str(l)]\n",
        "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
        "        # (approx. 2 lines)\n",
        "        v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)]/(1-beta1 ** t)\n",
        "        v_corrected[\"db\" + str(l)] = v[\"db\" + str(l)]/(1-beta1 ** t)\n",
        "\n",
        "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "        #(approx. 2 lines)\n",
        "        s[\"dW\" + str(l)] = beta2 * s[\"dW\" + str(l)] + ((1-beta2) * (grads[\"dW\" + str(l)]**2))\n",
        "        s[\"db\" + str(l)] = beta2 * s[\"db\" + str(l)] + ((1-beta2) * (grads[\"db\" + str(l)]**2))\n",
        "\n",
        "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "        # (approx. 2 lines)\n",
        "        s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)] / (1-beta2**t)\n",
        "        s_corrected[\"db\" + str(l)] = s[\"db\" + str(l)] / (1-beta2**t)\n",
        "\n",
        "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "        # (approx. 2 lines)\n",
        "        parameters[\"W\" + str(l)] -= learning_rate * v_corrected[\"dW\" + str(l)] / (np.sqrt(s_corrected[\"dW\" + str(l)]) + epsilon)\n",
        "        parameters[\"b\" + str(l)] -= learning_rate * v_corrected[\"db\" + str(l)] / (np.sqrt(s_corrected[\"db\" + str(l)]) + epsilon)\n",
        "\n",
        "    return parameters, v, s, v_corrected, s_corrected"
      ],
      "metadata": {
        "id": "2vsORxGaAabW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}